# 1、波士顿：线性回归（须知）

![image-20210524165623039](https://gitee.com/sheep-are-flying-in-the-sky/my-picture/raw/master/picture9/image-20210524165623039.png)



# 2、代码实现

## 2.1、导入依赖

~~~python
import torch                        # 导入：框架torch依赖
from torch import nn, optim         #torch 常用的包nn, optim
from sklearn import datasets        #导入数据集的模块
import matplotlib.pyplot as plt     #用于画图
~~~



## 2.2、加载数据集


```python
boston = datasets.load_boston()   # 获取到：波士顿（特征、房价）所有信息
```

## 2.3、筛序数据


```python
X = boston.data                # 存放：所有（特征）
y = boston.target              # 存放：房价（标签）

X = X[y < 50]                  # 筛选：去除房价 >= 50 的（特征）
y = y[y < 50]                  # 去除：房价 >= 50  （标签）
x = X[:,5]                     # 获取到：第5个特征的（506,）样本

#拓展第5特征维度：将array(506,) 转化为了torch.Size([506, 1])
x_expand  = torch.unsqueeze(torch.Tensor(x),dim=1)  
#拓展房价维度：将array(506,) 转化为了torch.Size([506, 1])
y_expand  = torch.unsqueeze(torch.Tensor(y),dim=1)  
```

## 2.4、划分数据集（未使用）



## 2.5、构建网络


```python
# 调用Net(x)会调用其call方法, call方法会调用：forward方法
class Net(nn.Module):
    def __init__(self):                
        super(Net, self).__init__()    #调用父类：构造方法
        # 输入的二维张量（神经元1个）， 输出的二维张量（神经元一个）， 会线性回归实现y = 权重 * 特征 （累加和）
        # (1, 1) ：传入样本的（特征）1个， 输出1个值（房价）
        self.linear = nn.Linear(1, 1)  
                                                                    
    def forward(self, x):             #向前转播，x为样本特征
        output = self.linear(x)       # 线性回归函数， 会线性回归实现y = 权重 * 特征 （累加和）
        return output                 # output： 预测值（房价）
```

## 2.6、定义：优化器和损失函数


```python
net = Net()                # 创建Net类对象 
criterion = nn.MSELoss()   # 线性回归的损失函数：计算（预测值）与（真实值之差）的（平方和）的（平均值），得到（一组合适的）theta
optimizer = optim.SGD(net.parameters(), lr=1e-4)  #随机梯度下降优化器（param是：线性回归函数的参数）
```

## 2.7、定义：绘图函数


```python
def draw(output, loss):
    plt.cla()                   #Clear axis即清除当前图形中的当前活动轴。其他轴不受影响。
    plt.scatter(x_expand.numpy(), y_expand.numpy(), c='lightskyblue')#散点图 （原始特征， 真实值）
    plt.plot(x_expand.numpy(), output.data.numpy(), 'g*', lw = 5)   #线性回归直线图（原始特征，预测值）
    plt.text(3.9, -13, 'Loss=%s' %(loss.item()), fontdict={'size':20, 'color':'green'})
    plt.pause(0.005) # 暂停
```

## 2.8、训练网络


```python
inputs = x_expand       # 训练：第5特征
targets = y_expand      # 真实值：房价（标签）
def train(model, criterion, optimizer, epochs):  #参数（Net对象， 损失函数, 优化器, 训练次数）
    for epoch in range(epochs):    
        output = model(inputs)   #调用Net的call方法, Net的call里面调用Net的forward(前向传播)
        loss = criterion(output, targets)     #调用损失函数，传入预测值（和）真实值：求出误差，获取合适的theta
        
        #batch计算一次梯度，将进行一次梯度更新, 如果不将梯度清零的话，当前梯度会与上一个batch的数据相关
        optimizer.zero_grad()    # 清空梯度
        
        #optimizer只负责通过梯度下降进行优化，而不负责产生梯度，梯度是tensor.backward()方法产生的。
        loss.backward()          # 反向传播：计算得到每个参数的梯度值
        optimizer.step()         # 通过梯度下降：执行一步参数更新（optimizer.step()）
        
        if epoch % 100 == 0:     # 每计数100次，画一次图
            draw(output,loss) 
    return model, loss    
```

## 2.9、执行操作


```python
from time import perf_counter     # （时间模块）提供了各种与时间有关的功能， jupyter 中不加（未报错）
start = perf_counter()            # perf_counter() 返回性能计数器的值（以分秒为单位）
LR_model, loss = train(net, criterion, optimizer, 1000)  #训练
finish = perf_counter()           #获取：训练后的截止时间
time = finish - start             #计算：训练所用时间 = 截止时间 - 开始时间  
print("计算时间：%s" % time)
print("final loss：", loss.item())
print("weights", list(net.parameters())) #用有两个参数：特征theta1 （和） 截距
```




# 3、效果图

![image-20210524154822548](https://gitee.com/sheep-are-flying-in-the-sky/my-picture/raw/master/picture9/image-20210524154822548.png)

---



# 4、代码语句：剖析

## 4.1、导入的nn 和 optim 

~~~python
torch.nn包
	主要包含了用来搭建各个层的模块（Modules），比如全连接、二维卷积、池化等； torch.nn包中还包含了一系列有用的loss函数，这些函数也是在训练神经网络时必不可少的，比如CrossEntropyLoss、 MSELoss等。

orch.optim包
	主要包含了用来更新参数的优化算法，比如SGD、AdaGrad、RMSProp、 Adam等
~~~





## 4.2、torch.unsqueeze()

~~~python
1、作用：用来拓展维度

2、参数：（Tensor, dim） 
	2.1：Tensor     # 输入张量
    2.2: dim        # 需要拓展的维度数目

3、返回值：返回一个新的张量
~~~



## 4.3、PyTorch的nn.Linear（）

~~~python
1、作用：用于设置网络中的（全连接层）的，需要注意的是全连接层的输入与输出都是二维张量

2、参数：（in_features, out_features, bias=True）
    2.1: in_features指的是输入的二维张量的大小
    2.2: out_features指的是输出的二维张量的大小, 它也代表了该全连接层的神经元个数。
    2.3: bias默认为true，若为false则不进行学习

3、返回值：CLASS
~~~





## 4.4、nn.MSELoss（）

~~~python
1、概念：MSE: Mean Squared Error（均方误差）

2、作用：是（预测值）与（真实值之差）的（平方和）的（平均值）

3、参数: (reduce=true,size_average=true)
    3.1：如果redcue=true,那么loss返回的是标量; 
    3.2：size_average=True, 返回loss.mean();   
    
4、返回值： 可以是向量或者矩阵，i 是下标。
~~~



## 4.5、optim.SGD（）

~~~python
1、作用： torch.optim是神经网络优化器，使神经网络在我们的训练过程中快起来。 optim.SGD是 Stochastic Gradient Descent（随机梯度下降法） SGD是最基础的优化方法，普通的训练方法, 需要重复不断的把整套数据放入神经网络NN中训练。 这样消耗的计算资源会很大.当我们使用SGD会把数据拆分后再分批不断放入 NN 中计算. 每次使用批数据。 虽然不能反映整体数据的情况, 不过却很大程度上加速了 NN 的训练过程, 而且也不会丢失太多准确率。

2、参数：（model.parameters(),lr = 0.01, momentum = 0.9）
	2.1：model.parameters()  #获取model网络的参数，构建好神经网络后，网络的参数都保存在						     parameters()函数当中。
    
	2.2：lr是学习率  #（学习率较小）时，收敛到极值的速度较慢。（学习率较大）时，容易在搜							索过程中发生震荡。   
    
    2.3：momentum      #“冲量”这个概念源自于物理中的力学，表示力对时间的积累效应。
    
3、返回值 optimizer对象，这个对象能保存当前的参数状态并且基于计算梯度进行更新。
~~~

